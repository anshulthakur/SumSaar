[User]
I'm writing a tool for detecting similar stories across multiple news feeds and putting similar news items into a single bucket for further processing. What method do you think will work best?

[BingGPT]
That sounds like a fascinating project! Given your goal of grouping similar news stories across multiple feeds, the best approach would likely combine semantic similarity and topic modeling to ensure articles are clustered meaningfully. Here‚Äôs a breakdown of the most effective methods:

TF-IDF + Cosine Similarity ‚Äì This is a strong baseline where you vectorize articles and compare their similarity mathematically. Works well for text with overlapping words but struggles with paraphrased content.

Word Embeddings (Word2Vec, GloVe, FastText) ‚Äì These models assign words numerical meanings based on their context, making them useful for finding semantic similarity even when wording differs.

Topic Modeling (LDA) ‚Äì Latent Dirichlet Allocation groups articles based on underlying topics, helping sort news stories into buckets by themes rather than exact phrasing.

Jaccard Similarity on N-grams ‚Äì Useful for identifying duplicate or near-duplicate stories by comparing shared phrases.

Clustering Algorithms (K-Means, DBSCAN) ‚Äì These techniques group articles into clusters based on their feature representation, helping categorize related news articles dynamically.

If the feeds are diverse and news articles vary in wording, I‚Äôd suggest a combination of Word Embeddings + Clustering for semantic matching and TF-IDF for keyword-based similarity.

Do you want help with implementing this in code?

[User]
yes. So, I guess there can be some sort of pipeline where we can do multiple such things in steps and finally arrive at a bucket?

[BingGPT]
Exactly! You can build a multi-step processing pipeline where each stage refines the grouping of news articles to ensure that only similar stories end up in the same bucket. Here's a possible structure for your pipeline:

1. Preprocessing
Text Cleaning: Remove stopwords, punctuation, and special characters.

Tokenization: Break text into meaningful words or phrases.

Stemming/Lemmatization: Normalize words to their base forms (e.g., "running" ‚Üí "run").

2. Feature Extraction
TF-IDF Vectorization: Extract keyword-based features.

Word Embeddings: Convert text into dense vector representations (Word2Vec, GloVe, FastText).

Topic Modeling (LDA): Identify underlying themes in each article.

3. Similarity Computation
Cosine Similarity (TF-IDF & Word Embeddings): Compute similarity scores between articles.

Jaccard Similarity (N-Grams): Detect overlapping phrases.

4. Clustering & Bucketing
Hierarchical or K-Means Clustering: Group articles based on feature similarity.

DBSCAN: Detect dense clusters of related news items.

Threshold-Based Filtering: Discard loosely related articles.

5. Post-processing & Validation
Manual Review (if needed): Fine-tune clustering thresholds.

Scoring & Ranking: Sort stories by relevance within a bucket.

Would you like a code snippet to help implement this in Python?


********************

ANOTHER DAY. ANOTHER RESTART

***********************

without using LLMs, how can we determine algorithmically if two textual articles of considerable length are discussing the same thing, but maybe in different words?
Great question! Without relying on LLMs, you can still determine if two articles are discussing the same topic algorithmically by using a combination of natural language processing (NLP) techniques and statistical methods. Here‚Äôs a structured approach:

1. Preprocessing
Tokenization: Break down both texts into words or phrases.

Stopword Removal: Remove common words like ‚Äúthe,‚Äù ‚Äúis,‚Äù ‚Äúand,‚Äù etc.

Stemming/Lemmatization: Reduce words to their base forms (e.g., ‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù).

Part-of-Speech Tagging: Identify nouns, verbs, and other word categories.

2. Vector Representation
TF-IDF (Term Frequency-Inverse Document Frequency): Represents each article as a weighted vector based on term importance.

Word Embeddings (non-LLM-based), like Word2Vec or GloVe: Maps words to a multidimensional space where semantic similarity is preserved.

Bag-of-Words (BoW): Represents documents as word frequency vectors.

3. Semantic Similarity
Cosine Similarity: Measures the cosine angle between two text vectors to identify overlap.

Jaccard Similarity: Compares sets of words appearing in both articles.

Latent Semantic Analysis (LSA): Uses singular value decomposition (SVD) to uncover deeper relationships.

4. Entity & Topic Extraction
Named Entity Recognition (NER): Identifies and compares important entities (people, places, events) in both texts.

Topic Modeling (LDA - Latent Dirichlet Allocation): Extracts major topics and compares their distributions across texts.

5. Structural & Contextual Analysis
Dependency Parsing: Understand sentence structures and compare syntactic similarities.

Text Summarization & Comparison: Generate summaries using extractive methods and compare them.

Event Extraction: Identify key events and match them between articles.

6. Graph-Based Approaches
Knowledge Graphs: Identify connections between concepts mentioned in both articles.

Text Graph Similarity: Represent words as nodes and relationships as edges; compare graph structures.

Using a mix of these techniques, you can build an effective similarity detection system without needing LLMs! Would you like a more detailed breakdown of any of these methods?



## Another day, with something already implemented. But, using LDA gave many false groupings, so I went back and asked:

I want to compare the texts of two articles and see if they're covering the same topic or not. If they're covering the same topic, 
it is quite possible that both of them essentially contain the same information worded differently. 
However, it is also possible that each contains facts which are not present in the other article. 
In terms of sets, I want to know if one article is a subset of the other, a superset of the other, or is there a large 
intersection among the two with some non-empty difference sets as well. What I don't want to do is to club two articles together when they are not covering the same news.
For example, one article may be covering the news of a building collapse in Delhi an the casualties, 
while the other talks of another collapse in UP. But, the two events are separate. 
So, I want to be able to distinguish between the two reliably
Your goal is clear‚Äîyou want a robust way to determine topic similarity, while also analyzing information coverage (subset, superset, 
or partial intersection) between articles. 
You also need a system that avoids false matches, ensuring that similar wording does not mistakenly categorize two distinct events as the same.

Approach
To achieve this, we‚Äôll use a two-step verification process: 
1Ô∏è‚É£ Topic Classification: First, determine whether two articles are about the same event using semantic similarity & entity recognition. 
2Ô∏è‚É£ Content Coverage Analysis: If they cover the same event, check if one article is a subset, superset, 
or has partial overlap using set-based comparisons.

Step 1: Topic Classification (Avoid False Matches)
Before comparing content coverage, we first ensure both articles actually discuss the same event using: 
‚úÖ Named Entity Recognition (NER) ‚Äì Extract people, locations, dates, key terms and compare overlaps. 
‚úÖ Latent Semantic Analysis (LSA) or SBERT ‚Äì Compare topic embeddings to verify event similarity. 
‚úÖ Event Extraction (OpenIE or spaCy) ‚Äì Identify action-oriented phrases 
(e.g., "building collapsed", "earthquake struck") to ensure same event focus.

If NER and event similarity confirm a match, proceed to Step 2.

Step 2: Content Coverage Analysis (Subset, Superset, or Partial Overlap)
Once we‚Äôre sure the articles cover the same event, we measure information coverage using: 
‚úÖ Jaccard Set Comparison ‚Äì Compare word/token sets and classify:

Subset (A ‚äÜ B) ‚Üí Article A contains a strict subset of facts from B.

Superset (A ‚äá B) ‚Üí Article A contains additional information beyond B.

Intersection (A ‚à© B) ‚Üí Both contain overlapping facts, but neither is strictly contained in the other. 
‚úÖ TF-IDF + Concept Matching ‚Äì Identify unique terms in each article, helping flag exclusive details missing from one but present in the other.

How This Helps
üîπ Avoids false grouping (e.g., two distinct building collapses won‚Äôt be confused). 
üîπ Captures nuance‚Äîdetects if an article misses key details or if one is just a reworded version of another. 
üîπ Detects factual differences‚Äîensuring that partial matches do not imply complete topic overlap.

OK.
After some wrangling with the LLM and some code, here's what I've observed.
1. NER extracts labels, but not everything. For example, in an article about gold prices soaring, it extracted everything but gold (despite having the label OBJECT/PRODUCT for it.)
2. The LLM throws a lot of options my way when I talk to it, not all are good. So, I go with the ones which I feel are nearest to what
I've done till that point, and see if it works.
3. The options can make it feel daunting and overwhelming. It is so easy to get lost in the weeds. So, I'm reorienting myself towards completing the minimal prototype pipeline and then fixing parts of it incrementally.

For the 3rd point above, here's what I've done so far:
‚úÖ Download RSS feeds and save to a JSON file. Define the structure in pydantic to allow some constraint on what I'm doing. 
This leads to a lot of Pydantic models, but it's easier to refer to the model than put a print on the JSON object to see what it brings.
‚úÖ Download is using newspaper4k which does not directly handle javascript, so integrated with playwright headless browsing. The code is flaky, 
but exceptions are being handled (and article downloads skipped)
‚úÖ Another set of methods to parse the saved JSON and organize the articles in terms of similarity. So far, Latent Semantic Analysis seems 
to work best, and has decent speed - especially when compared to the LLM models. Latent Dirichlet Allocation (LDA) doesn't work nicely.
‚úÖ Finally, another script currently utilizes the output of LSA similarity to compact and rewrite the set of related articles 
into a single one using LLMs. Here, the speed is slow, and when there are many similar articles on the same topic, the length may become a constraint.
Further, the notion of 'updates' in NEWS (there was a terrorist attack in Pahalgam yesterday, 25 articles on it, each one evolving) is missing. 
The LLM may put all the figures inside the article and make it confusing.

But, let's put up a dirty web-page to browse and read these stories (using some python package instead of making it big with Flask/Django).